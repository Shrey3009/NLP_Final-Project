# ğŸ“˜ EduGrade: Retrieval-Augmented Short-Answer Grading
End-to-end system for retrieving scientific context, generating reference answers, and grading student short responses with a RoBERTa classifier.

Project structure includes:
- `final_project.ipynb` â€” complete pipeline  
- `science_kb_FINAL.json` â€” knowledge-base passages  
- `science_kb_FINAL.npy` â€” MPNet embeddings  
- `science_kb_FINAL.faiss` â€” FAISS index
- `baseline_grader/` â€” not included due to size; required for full reproduction  

## âš™ï¸ Environment

The simplest way to run the project is **Google Colab**, since all paths in the notebook are already configured for Colab execution.

If running locally, update the file paths in the notebook.

### Requirements
- Python 3.9+
- transformers  
- datasets  
- sentence-transformers  
- faiss-cpu  
- openai  
- numpy  
- scikit-learn  

Install:
```bash
pip install transformers datasets sentence-transformers faiss-cpu openai scikit-learn
```
## ğŸš€ Running the Pipeline
1. Open Google Colab.  
2. Upload:
   - `science_kb_FINAL.json`
   - `science_kb_FINAL.npy`
   - `science_kb_FINAL.faiss`
3. Add your OpenAI API key in the corresponding notebook cell.  
4. Run the notebook top-to-bottom.

The pipeline performs:
- Knowledge Base Construction
- Dataset Preparation
- Baseline Grader Training (RoBERTa)
- Retriever Index Construction
- Retrieval
- RAG Reference Generation
- Grading with RoBERTa
- Evaluation (Baseline vs RAG)
- Semantic Quality Scoring

---

## ğŸ§© System Overview

### Baseline Grader Training
A RoBERTa classifier is fine-tuned using gold reference answers.
This establishes the baseline grader used for all later evaluations.

Input format during training:
[QUESTION] ...
[REFERENCE] gold_reference
[ANSWER] student_answer

### Retrieval
MPNet encodes every passage in the synthetic knowledge base.

FAISS retrieves the top-k most relevant passages for a given question.

### RAG Reference Generation
GPT-4o-mini generates a concise teacher-style reference answer using only the retrieved passages.

QUESTION: q
PASSAGES:
- p1
- p2
- p3


### RAG-Based Grading
The same RoBERTa model is used again, but this time the reference is generated by RAG:

[QUESTION] q  
[REFERENCE] rag_reference  
[ANSWER] student_answer

## ğŸ“Š Evaluation Summary

### Baseline Grader (Gold Reference Mode)
- Accuracy â‰ˆ **0.60**
- Macro-F1 â‰ˆ **0.46**

### RAG Pipeline (RAG Reference Mode)
- Accuracy â‰ˆ **0.613**
- Macro-F1 â‰ˆ **0.44**

### RAG Answer Quality
- **BERTScore (RAG vs Gold) â‰ˆ 0.878**

---

## ğŸ¯ End-to-End Demonstration
The notebook includes complete examples showing:
- retrieved passages  
- generated RAG references  
- final predicted labels produced by the trained RoBERTa model

---

## ğŸ“ Notes
- All retrieval files (`.json`, `.npy`, `.faiss`) are included.  
- The RoBERTa grading model is not uploaded due to size constraints.  
- To reproduce predictions exactly, place the `baseline_grader` folder at the expected notebook path.

---

## ğŸ”§ Future Extensions
- Retrain the classifier using RAG-generated references to reduce style mismatch.  
- Add cross-encoder reranking or passage merging to improve retrieval quality.  
- Expand the synthetic knowledge base.  
- Evaluate grading robustness on real student responses.

